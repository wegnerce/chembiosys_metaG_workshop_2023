{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1868c788-2281-461d-b1e5-58e5e65764c9",
   "metadata": {},
   "source": [
    "# ChemBioSys and AquaDiva | Genome-resolved metagenomics workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e875cef-5e5e-4fef-af3c-63ee63a792a0",
   "metadata": {},
   "source": [
    "## Session 02 | Taxonomic profiling of metagenome datasets\n",
    "\n",
    "As said before, you will be working with two mock datasets that have been generated in silico. Starting from an unknown number of genomes, we have simulated two sequencing runs, and in these two runs the original genomes are differently covered, they differ in their abundance between the two datasets. This will become relevant further down the road, once we start doing assemblies and genome binning\n",
    "\n",
    "Before we assembly our sequence data we want to get a first glimpse into our two datasets. Having this information available can help us getting an idea what we can expect from our downstream metagenome assembly.\n",
    "\n",
    "For the taxonomic profiling of short-read metagenome datasets, plenty of tools are available, a few are listed here:\n",
    "\n",
    "|  **Tool**              | **Strategy**                                              |  **Reference** |\n",
    "|------------------------|-----------------------------------------------------------|----------------|\n",
    "|   Kraken/Kraken2       | k-mer profile matching                                    | [Wood et al., 2019](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1891-0) |\n",
    "|   MetaPhlAn4           | unique SGB marker genes                                   | [Blanco-Miguez et al., 2023](https://www.nature.com/articles/s41587-023-01688-w) |\n",
    "|   kaiju                | translated search, substring matching                     | [Menzel et al., 2016](https://www.nature.com/articles/ncomms11257) |\n",
    "|   diamond/blast/megan  | local alignment searches, LCA-based taxonomic assignment  | [Bagci et al., 2021](https://currentprotocols.onlinelibrary.wiley.com/doi/full/10.1002/cpz1.59) |\n",
    "\n",
    "The workflows for `Kraken`and `Kaiju`are exemplarily explained here.\n",
    "\n",
    "### Taxonomic profiling with Kraken\n",
    "\n",
    "<img src=\"img/kraken.png\" alt=\"Kraken\" width=\"750\"/>\n",
    "\n",
    "<font size=\"2\"> The k-mer matching based algorithm underlying Kraken, [_Wood and Salzberg, 2014_](https://genomebiology.biomedcentral.com/articles/10.1186/gb-2014-15-3-r46) </font>\n",
    "\n",
    "For any query sequence, `Kraken`computes all possible k-mers of a defined length. k-mers are substrings that are contained in a sequence of length k. Imagine the sequence `ATGCGTGA`, let's identify all 3-mers (k=3).\n",
    "\n",
    "```\n",
    "ATG\n",
    "TGC\n",
    "CGT\n",
    "GTG\n",
    "TGA\n",
    "```\n",
    "\n",
    "`Kraken`uses by default a k-mer length of 31, to classify a sequence, each k-mer in a sequence is mapped to the lowest common ancestor (LCA) of the genomes that contain that k-mer. The taxa associated with the sequence‚Äôs k-mers, as well as the taxa‚Äôs ancestors form a tree, which is used for classification. \n",
    "\n",
    "### Taxonomic profiling with kaiju\n",
    "\n",
    "<img src=\"img/kaiju.png\" alt=\"Kaiju\" width=\"750\"/>\n",
    "\n",
    "<font size=\"2\"> The substring matching approach of Kaiju, [_Menzel et al., 2016_](https://www.nature.com/articles/ncomms11257) </font>\n",
    "\n",
    "In the case of `kaiju`, a query sequence is first translated into the six possible reading frames. The resulting amino acid sequences are split into fragments at stop codons. This population of fragments is then either sorted by length or BLOSUM62 score (this score is calculated based on the most likely substitution at each position). \n",
    "\n",
    "The sorted fragments are then searched against a reference databases, looking for either exact matches, or matches with the highest BLOSUM62 score. In case of the latter, exact matches of a minimum seed length (length = 7) are extended in both directions allowing a certain number of mismatches. Taxonomic assignment is based on the taxonomic identity of the database sequence containing the best match. If more than one database sequence contains the match, the assignment is based on LCA.\n",
    "\n",
    "For the workshop we make use of `kaiju` because of its smaller memory footprint (and because of the cooler logo and subtle reference to Asian monster movies... üòÅ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa65de-8d45-4d5e-9613-7c0368b690b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alright we make sure we are in the workshop directory\n",
    "cd ~/workshop\n",
    "### We switch to the directory 00_READS, which contains our two datasets,\n",
    "### and create a directory for the kaiju output we are about to generate\n",
    "mkdir kaiju_out\n",
    "### Now we call kaiju\n",
    "kaiju-multi -z 25 -t nodes.dmp -f kaiju_db.fmi -i sample1_R1.fastq,sample2_R1.fastq,sample3_R1.fastq -j sample1_R2.fastq,sample2_R2.fastq,sample3_R2.fastq  -o sample1.out,sample2.out,sample3.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce775a-d438-417f-a5b9-5b1aba40f661",
   "metadata": {},
   "source": [
    "`kaiju-multi`: run kaiju on multiple datasets at the same time\n",
    "`-z`: with this option we can specify the number of threads to use    \n",
    "`-t`: nodes.dmp, contains information about taxonomy nodes in the [`NCBI taxonomy`](https://www.ncbi.nlm.nih.gov/guide/taxonomy/), each taxonomy node has a unique identifier, nodes.dmp contains this identifier, as well as information about parent nodes, taxonomic rank, etc.    \n",
    "`-f`, `-j`: paths to forward and reverse read files, files are given as comma-separated list    \n",
    "`-o`: specified path for storing output    \n",
    "\n",
    "Once `kaiju` is done, we want to summarize the taxonomic assignments for visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4197a69-8281-4e3b-a98c-0e89d27523b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We convert the output from kaiju into so called krona plots\n",
    "kaiju2krona -t nodes.dmp -n names.dmp -i kaiju.out -o kaiju.out.krona\n",
    "ktImportText -o kaiju.out.html kaiju.out.krona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4780f941-e2b3-4ac5-afcd-693d519ef420",
   "metadata": {},
   "source": [
    "`-t`: see above   \n",
    "`-n`: names.dmp links the information from nodes.dmp (taxonomic identifiers) to unique taxonomy names, nodes.dmp and names.dmp are needed to summarize the taxonomic profiles that we obtained from kaiju         \n",
    "`-i`: kaiju output used as input for the conversion   \n",
    "`-o`: specified paths for storing output    \n",
    "\n",
    "---\n",
    "üîß**TASK**\n",
    "\n",
    "Download the krona plots, open them in a browser and check out the taxonomic composition of the two datasets. Do they differ, do you see dominating taxa?\n",
    "\n",
    "---\n",
    "\n",
    "When doing taxonomic profiling (or any kind of profiling) it is important to know what proportion of your data got successfully assigned. The krona plots show you the number of assigned reads for the different taxonomic ranks, but how many sequences do we have in our datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d10bc6-dbcc-4743-b118-60173e3e0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assuming we are still in \"00_READS\" we can check the number of reads as follows\n",
    "expr $(cat dataset_A_1.fastq | wc -l) / 4\n",
    "expr $(cat dataset_B_1.fastq | wc -l) / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd5e6a-859c-4c5e-8c63-9212e45e3475",
   "metadata": {},
   "source": [
    "`expr`: tool that allows you to evaluate expressions, here the number of lines in the .fastq file divided by 4    \n",
    "`$(cat dataset_A_1.fastq| wc -l)`: the .fastq file is opened with `cat` and the number of lines counted with `wc -l`    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ede6f9-313d-40bc-967c-1c6d5a6bae72",
   "metadata": {},
   "source": [
    "## Background | Metagenomics and its impact on public databases\n",
    "\n",
    "Genome-resolved metagenomics became very mainstream over the last years and it is meanwhile not uncommon to see studies that deposit thousands of genomes recovered from metagenome datasets. \n",
    "\n",
    "Take a look at the figure below, which shows you the genome content in ['IMG/GOLD'](https://gold.jgi.doe.gov/index) as of 2016. Until 2016, databases were heavily dominated by genomes originating from isolates. This drastically changed due to genome-resolved metagenomics.\n",
    "\n",
    "<img src=\"img/magsag.png\" alt=\"Genome content in IMG/GOLD\" width=\"750\"/>\n",
    "\n",
    "<font size=\"2\"> Genomes in IMG/GOLD as of 2017. [_Bowers et al., 2017_](http://dx.doi.org/10.1038/nbt.3893) </font>\n",
    "\n",
    "Meanwhile, [`NCBI Genomes`](https://www.ncbi.nlm.nih.gov/assembly/) contains around 1.7M bacterial genomes and a lot of them originate from metagenomic studies, but only a little over 40k genomes are considered complete.\n",
    "\n",
    "## Session 02 | Estimating metagenome coverage\n",
    "\n",
    "Coming back to our datasets, after we got a bit of an idea what taxonomic groups are in our datasets, we now want to check how well our datasets cover sequence diversity. What does that mean or why is that important? We ultimately want to recover near-complete genomes. For that we need enough sequencing data to assemble the respective genomes successfully. If we have highly and lowly abundant taxa present, they are represented by differently sized portions of our sequencing data. \n",
    "\n",
    "Imagine you are after the genome of a taxon that is known to feature rather big genomes, but the abundance of this taxon in your dataset is rather low. In such a case it is important to sequence \"deep\" enough (generate sufficient sequencing data) for downstream assembly.\n",
    "\n",
    "We can estimate how well our datasets cover sequence diversity by means of k-mer based rarefaction analysis. We learned about k-mers before. Rarefaction is used in Ecology to assess richness. In microbial ecology, we for instance use rarefaction to assess species richness based on 16S rRNA gene sequencing data. We continuously subsample the sequences, starting from e.g. 0 sequences and then increment the number of sampled sequences by let's say 10, and count the number of species we find. This relationship between sampled sequences and number of sequences can then be plotted. Rarefaction curves usually rise sharply and saturate once richness is nearly covered.\n",
    "\n",
    "We can do the same with metagenomic datasets. We sample our reads and count the number of unique k-mers. For this we make use of [`nonpareil`](https://github.com/lmrodriguezr/nonpareil). Based on k-mer diversity in a dataset, `nonpareil` estimates how well the corresponding dataset covers sequence diversity. At the same time it gives you an idea how much data you need to reach good coverage. Below is some exemplary output.\n",
    "\n",
    "<img src=\"img/nonpareil_cov_estimates.png\" alt=\"Metagenome coverage estimation taken from Wegner et al., 2023\" width=\"400\"/>\n",
    "\n",
    "<font size=\"2\"> Metagenome coverage estimation taken from [_Wegner et al., 2023_](https://www.biorxiv.org/content/10.1101/2022.09.23.509128v1). </font>\n",
    "\n",
    "OK, let's check our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253a483-1df5-4f6c-8fc0-02cb1719662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assuming we are still in \"00_READS\"\n",
    "### Create a directory for nonpareil output\n",
    "mkdir nonpareil_out\n",
    "### We interleave the pairs of reads so that we can run nonpareil\n",
    "### over forward and reverse reads\n",
    "cat dataset_A_[12].fastq > dataset_A_interleaved.fastq\n",
    "nonpareil -s dataset_A_interleaved.fastq -T kmer -f fastq -b nonpareil_out/dataset_A -t 6\n",
    "### We take a look at the nonpareil output\n",
    "cat nonpareil_out/dataset_A.npo | awk -F\"\\t\" '{print $1 \"\\t\" $2 \"\\t\" \"$3\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385cb5c-e183-49d5-968a-14eb297ac579",
   "metadata": {},
   "source": [
    "`cat`: command that prints and concatenates files    \n",
    "`[12]`: we make use of wildcards for the concatenation    \n",
    "`-s`: input file for `Nonpareil`    \n",
    "`-T`: method for coverage estimation, we use k-mers (length = 21)    \n",
    "`-f`: format of the input file    \n",
    "`-b`: specified output path    \n",
    "`-t`: number of threads to use in parallel, here 6    \n",
    "`awk`: commandline programming language for text file manipulation    \n",
    "`-F`: field separator of input file, here `TAB`   \n",
    "`print`: print selected columns here, columns 1, 2, and 3\n",
    "\n",
    "The first column indicates the sequencing effort (= amount of data sampled), the second the average coverage, and the third one the standard deviation of the coverage. We see that in the case of dataset A, sequence diversity is covered to almost 100% and that 90% coverage would have been reached with less sequencing effort.\n",
    "\n",
    "---\n",
    "üîß**TASK**\n",
    "\n",
    "Check dataset B in the same way.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87fe65c-fb51-4747-b647-0b6a2d2dba96",
   "metadata": {},
   "source": [
    "## Background | A look into the theory behind (meta)genome assembly\n",
    "\n",
    "Alright, after looking at sequence data formats, checking out exemplary QC reports, taxonomic profiling, and metagenome coverage estimation, we now move on talking about metagenome assembly. \n",
    "\n",
    "Metagenome datasets usually originate from the sequencing of environmental genomic DNA, which means that sequenced DNA fragments originate from many different, taxonomically diverse microbes. With reference to metagenome assembly this kind of sounds like a problem and yes it is a problem. High-throughput sequencing platforms generate between 100Ks and 1Bs of sequence reads. How do we stitch them together?\n",
    "\n",
    "This question was actually, kind of answered quite a while ago, namely in the city of K√∂nigsberg, by the mathematician Leonhard Euler, who founded [graph theory](https://en.wikipedia.org/wiki/Graph_theory) while working on the K√∂nigsberg problem. \n",
    "\n",
    "The K√∂nigsberg problem refers to the seven bridges of K√∂nigsberg. In the 18<sup>th</sup> century the residents of K√∂nigsberg wondered whether every part of the city could be visited by walking each bridge only once and returning to ones starting point. Eulers idea was as simple as brilliant. He imagined every landmass of the city as a node, and every bridge as an edge, resulting in a network of nodes connected by edges. The original question was thus simplified to finding a path in the network, which uses every edge once.\n",
    "\n",
    "<img src=\"img/konigsberg.png\" alt=\"The bridges of K√∂nigsberg problem\" width=\"750\"/>\n",
    "\n",
    "<font size=\"2\"> The bridges of K√∂nigsberg problem? [_Compeau et al., 2011_](https://www.nature.com/articles/nbt.2023) </font>\n",
    "\n",
    "This idea was taken further by Nicolas de Bruijn, who invented de Bruijn graphs. A de Bruijn graph is a cyclic sequence of letters taken from a given alphabet for which every possible word of a certain length (k) appears as a string of consecutive characters in the cyclic sequence exactly once. \n",
    "\n",
    "Transferring this idea to metagenome assembly means the following: \n",
    "\n",
    "1. we split our sequenced metagenome reads into k-mers with a length k,\n",
    "2. we plot deduced (k-1)-mers as nodes,\n",
    "3. and connect them using the k-mers as edges.\n",
    "\n",
    "Now we walk the graph and use every k-mer only once. A simple example is shown below.\n",
    "\n",
    "From a computational perspective, this is still demanding, but it is a solvable problem. Most currently used (meta)genome assemblers are based on de Bruijn graphs, and while first assemblers had high computational requirements, primarily RAM for storing intermediate de Bruijn graphs, recent assemblers are able to run on almost standard, everyday hardware.\n",
    "\n",
    "<img src=\"img/seq.png\" alt=\"K-mers and (K-1)-mers\" width=\"500\"/>\n",
    "<img src=\"img/debruijn.png\" alt=\"De Bruijn graphs\" width=\"750\"/>\n",
    "<font size=\"2\"> Example, de Bruijn graph-based assembly. </font>\n",
    "\n",
    "---\n",
    "‚ùì**QUESTION**\n",
    "\n",
    "This sounds so great and simple, why do we then struggle often with getting good, complete, and closed assemblies? How can we solve these problems?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b4d61-87a0-4309-8acc-dd261f8c07d0",
   "metadata": {},
   "source": [
    "## Session 02 | Metagenome co-assembly with MEGAHIT and METASPADES\n",
    "\n",
    "Let's assemble our datasets. We will use two different assemblers, [`MEGAHIT`](https://github.com/voutcn/megahit) and [`SPADES`](https://cab.spbu.ru/software/spades/). `MEGAHIT` is a very fast, memory-efficient short-read assembler, while `SPADES` is commonly considered the \"best\" short-read assembler available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6d373-7511-43cc-98c3-4926fa2e65e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We switch to our workshop folder and activate the needed conda environment\n",
    "cd ~/workshop\n",
    "conda activate assemblers\n",
    "### We first use MEGAHIT to co-assemble our two datasets ...\n",
    "megahit -1 ./00_READS/dataset_A_1.fastq.gz,./00_READS/dataset_B_1.fastq.gz  -2 ./00_READS/dataset_A_2.fastq.gz,./00_READS/dataset_B_2.fastq.gz -m 0.05 -t 8 -o ./02_ASSEMBLY_BINNING/assembly_megahit\n",
    "### ... and then SPADES in metagenome mode\n",
    "cat ./00_READS/*1.fastq.gz > ./00_READS/merged_1.fastq.gz\n",
    "cat ./00_READS/*2.fastq.gz > ./00_READS/merged_2.fastq.gz\n",
    "spades.py -t 8 --only-assembler -o ./02_ASSEMBLY_BINNING/assembly_spades --meta -1 ./00_READS/merged_1.fastq.gz -2 ./00_READS/merged_2.fastq.gz \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de9272-098a-4964-bc9b-657bbf370224",
   "metadata": {},
   "source": [
    "`--presets`: MEGAHIT comes with different k-mer presets dependent on the complexity of the metagenome    \n",
    "`-t`: We specify eight threads for the assembly, time is money, but we also do not want to block all our server capacity    \n",
    "`-1 / -2`: Forward and reverse reads to assemble    \n",
    "`--meta`: Similar to _MEGAHIT_, _SPADES_ comes with presets, --meta (surprise) is the preset for metagenomes    \n",
    "`-o`: output folders    \n",
    "`--only-assembler`: SPADES usually starts the assembly with a very thorough QC based on provided .fastq files    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a1972c-2721-4344-9a8f-3cbac6a9fcfe",
   "metadata": {},
   "source": [
    "Which assembler did a better job, how can we assess this, what are parameters/variables to look at? We use [`QUAST`](https://github.com/ablab/quast) to check out our assemblies.\n",
    "\n",
    "---\n",
    "‚ùì/üîß**QUESTION/TASK**\n",
    "\n",
    "* Use QUAST to get some basic statistics about the assembled sets of contigs.\n",
    "* What parameters are calculated (check N50, L50), how can they help you judge the performance of the assemblers?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b385a-491a-4395-945b-a45480fa66e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We switch the conda environment and use quast.py to check the assemblies\n",
    "conda deactivate\n",
    "conda activate metawrap\n",
    "### We check first the MEGAHIT assembly and then the SPADES assembly\n",
    "quast.py -o ./02_ASSEMBLY_BINNING/assembly_megahit/quast ./02_ASSEMBLY_BINNING/assembly_megahit/final.contigs.fa\n",
    "quast.py -o ./02_ASSEMBLY_BINNING/assembly_spades/quast ./02_ASSEMBLY_BINNING/assembly_spades/contigs.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a92a54d-f817-490f-b76f-947ae9c79bc5",
   "metadata": {},
   "source": [
    "Used parameters:\n",
    "\n",
    "`-o`: we define an output folder, where `QUAST` should store its reports.\n",
    "\n",
    "### Estimating the number of genomes in our assemblies\n",
    "\n",
    "Once the assembly is done, one important question is - \"With how many genomes are we (potentially) dealing with in our assembly?\". In order to answer this question we make use of [`anvio`](http://merenlab.org/software/anvio/). _anvio_ is \"an open-source, community-driven analysis and visualization platform for microbial ‚Äòomics. It brings together many aspects of today‚Äôs cutting-edge strategies including genomics, metagenomics, metatranscriptomics, pangenomics, metapangenomics, phylogenomics, and microbial population genetics in an integrated and easy-to-use fashion through extensive interactive visualization capabilities\". `anvio` is one lighthouse example for a community-effort to make cutting edge methods available to a broad group of (non-nerdy) users. üíóüíóüíó\n",
    "\n",
    "---\n",
    "‚ùì/üîß**QUESTION/TASK**\n",
    "\n",
    "How many genomes can we expect from the assembly?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21191cae-c7da-4547-b1fe-ce187abd0ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### We use anvio to estimate the number of genomes in our two assemblies\n",
    "### anvio expects that are contig names are formatted in consistent manner\n",
    "### How do our two .fasta files with assembled contigs look?!\n",
    "conda deactivate\n",
    "conda activate anvio7\n",
    "head ./02_ASSEMBLY_BINNING/assembly_megahit/final.contigs.fa\n",
    "head ./02_ASSEMBLY_BINNING/assembly_spades/contigs.fasta\n",
    "### In both cases, the header includes information about the contig assembly,\n",
    "### like contig length etc.\n",
    "### Let's reformat them\n",
    "anvi-script-reformat-fasta --simplify-names -o ./02_ASSEMBLY_BINNING/assembly_megahit/contigs_renamed.fa ./02_ASSEMBLY_BINNING/assembly_megahit/final.contigs.fa\n",
    "anvi-script-reformat-fasta --simplify-names -o ./02_ASSEMBLY_BINNING/assembly_spades/contigs_renamed.fa ./02_ASSEMBLY_BINNING/assembly_spades/contigs.fasta\n",
    "### We generate so called contig databases (explained below!), ...\n",
    "anvi-gen-contigs-database -f ./02_ASSEMBLY_BINNING/assembly_megahit/contigs_renamed.fa -n mock_metagenome_megahit -o ./02_ASSEMBLY_BINNING/mock_metagenome_megahit-CONTIGS.db -T 6\n",
    "anvi-gen-contigs-database -f ./02_ASSEMBLY_BINNING/assembly_spades/contigs_renamed.fa -n mock_metagenome_spaded -o ./02_ASSEMBLY_BINNING/mock_metagenome_spades-CONTIGS.db -T 6\n",
    "### ... identify certain genes, add this information (more about that in a bit) ...,\n",
    "anvi-run-hmms -c ./02_ASSEMBLY_BINNING/mock_metagenome_megahit-CONTIGS.db -I Bacteria_71,Archaea_76,Ribosomal_RNA_16S -T 6\n",
    "anvi-run-hmms -c ./02_ASSEMBLY_BINNING/mock_metagenome_spades-CONTIGS.db -I Bacteria_71,Archaea_76,Ribosomal_RNA_16S -T 6\n",
    "### ... and summarize them!\n",
    "anvi-display-contigs-stats --report-as-text -o ./02_ASSEMBLY_BINNING/stats_mock_metagenome_megahit.txt ./02_ASSEMBLY_BINNING/mock_metagenome_megahit-CONTIGS.db\n",
    "anvi-display-contigs-stats --report-as-text -o ./02_ASSEMBLY_BINNING/stats_mock_metagenome_spades.txt ./02_ASSEMBLY_BINNING/mock_metagenome_spades-CONTIGS.db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c65d1b5-efc6-413b-ad86-62354b6bfc5b",
   "metadata": {},
   "source": [
    "`anvi-script-reformat-fasta --simplify-names`: anvio script to reformat contig names for a consistent nomenclature      \n",
    "`anvi-gen-contigs-database`: anvio works with SQL databases to store information about contigs (length, location of open reading frames, annotation data, ...), databases are generated by first predicting open reading frames     \n",
    "`anvi-run-hmms -c -I -T 6`: generated contig databases are screened for collections of single-copy-marker genes and 16S rRNA genes   \n",
    "`-f / -n /-o`: specify input, give the database a name, define output  \n",
    "`anvi-display-contigs-stats`: script that generates basic contig statistics (similar to `QUAST` in a way...) and that predicts the number of genomes present in the contigs database  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff0c1f4-1292-445c-a80c-fc53a369c9aa",
   "metadata": {},
   "source": [
    "---\n",
    "‚ùì**QUESTION**\n",
    "\n",
    "* Have a look at the generated stats .txt files, how many genomes can we expect?\n",
    "* What resources did anvio use for its estimation of the number of genomes?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079cd3b8-bc56-4e32-b2d6-26a8daa8c0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check out the stats\n",
    "less ./02_ASSEMBLY_BINNING/stats_mock_metagenome_megahit.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c8f4d7-05d9-4c8e-a950-c23718f8d72d",
   "metadata": {},
   "source": [
    "## Session 03 | Automated binning with METAWRAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a3bb1f-3f26-4f0f-9cb9-6241c4f74ef3",
   "metadata": {},
   "source": [
    "### Background | Introduction to binning\n",
    "\n",
    "We assembled contigs, and based on the carried out screening with `anvio` our assembly contains bacterial as well as archaeal MAGs (metagenome-assembled genomes). A MAG is commonly defined as a genome that is reconstructed/recovered from a metagenome through assembly and binning. We did the assembly, what about binning? Binning aims at identifying and putting together contigs that belong to a population, a group of co-existing microbes whose genomes are sufficiently similar that genome assemblages from this population map to the same reference genome.\n",
    "\n",
    "<img src=\"img/binning_lego1.png\" alt=\"Assembly done\" width=\"500\"/>\n",
    "\n",
    "<font size=\"2\"> We did the assembly, binning is next. ¬© Rena Sophie Andr√§s </font>\n",
    "\n",
    "What kind of information can we use to identify populations of contigs that belong together?\n",
    "\n",
    "_Sequence composition-based:_\n",
    "* GC-content\n",
    "* k-mer frequencies\n",
    "\n",
    "_Reference-based:_\n",
    "* presence/absence of marker genes\n",
    "\n",
    "_Intra-sample coherence:_\n",
    "* differential coverage information\n",
    "\n",
    "**¬ª¬ª IDEALLY WE WANT TO COMBINE AS MUCH INFORMATION AS POSSIBLE ¬´¬´**\n",
    "\n",
    "In the metagenomics world people compare binning often with either confetti or LEGO, we are obviously biased and prefer LEGO üòÅ!\n",
    "\n",
    "<img src=\"img/binning_lego2.png\" alt=\"Assembly done\" width=\"500\"/>\n",
    "\n",
    "<font size=\"2\"> LEGO is a great analogy in the context of metagenomics. ¬© Rena Sophie Andr√§s </font>\n",
    "\n",
    "There are many different LEGO themes/lines (think about LEGO City, Classic, Technic, ...). You can imagine genomes as finished LEGO sets, and assemblies as a mess of disassembled sets. Every brick in this mess represents one contig. Now we sort them and put them back together, we bin them, by color, form, theme, etc..\n",
    "\n",
    "### Binning with METAWRAP\n",
    "\n",
    "For the binning process we will use [`METAWRAP`](https://github.com/bxlab/metaWRAP), or to be precise its implemented binning module. `METAWRAP` is a tool that \"wraps\" many different pieces of software together in the context of genome-resolved metagenomics. The strength of `METAWRAP` regarding binning is that it combines three individual binning tools:\n",
    "\n",
    "* [`CONCOCT`](https://github.com/BinPro/CONCOCT)\n",
    "* [`MAXBIN2`](https://sourceforge.net/projects/maxbin2/)\n",
    "* [`METABAT2`](https://bitbucket.org/berkeleylab/metabat/src/master/)\n",
    "\n",
    "All three make use of differential coverage information and sequence composition based on k-mer frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05c6a51-8355-4b55-bdf5-e3e78f58fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We activate the metawrap conda environment and get going\n",
    "conda deactivate\n",
    "conda activate metawrap\n",
    "metawrap binning -o ./02_ASSEMBLY_BINNING/initial_binning_spades_metawrap -t 6 -a ./02_ASSEMBLY_BINNING/assembly_spades/contigs.fasta --metabat2 --maxbin2 --concoct ./00_READS/*.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb923468-3a44-407a-bba7-705be40a7e73",
   "metadata": {},
   "source": [
    "`-`: output folder, `METAWRAP` will generate one folder with bins each, for `CONCOCT`, `MAXBIN2`, and `METABAT2`    \n",
    "`-t`: we specify 6 threads to speed up the processing    \n",
    "`-a`: the assembled contigs that should be used for the binning    \n",
    "`--concoct`, `--maxbin2`, and `--metabat2`: Binning is done using these three tools    \n",
    "\n",
    "If you check the defined output folder, you see the three output folders and you can see that the three tools identified different numbers of bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65af861-87c2-4a10-8c04-22af3c6cf40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We count the number of bins in each output folder\n",
    "ls ./02_ASSEMBLY_BINNING/initial_binning_spades_metawrap/concoct_bins/*.fa | wc -l\n",
    "ls ./02_ASSEMBLY_BINNING/initial_binning_spades_metawrap/maxbin2_bins/*.fa | wc -l\n",
    "ls ./02_ASSEMBLY_BINNING/initial_binning_spades_metawrap/metabat2_bins/*.fa | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db730fe0-6fdd-47cf-98ba-c0ca4ef1eab2",
   "metadata": {},
   "source": [
    "`ls`: we list only .fa files (using the `*` wildcard)...    \n",
    "`wc -l`: ... and count the number of files in the individual output folders    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0719f8f7-670d-4cc1-a17f-16134e2a1e2c",
   "metadata": {},
   "source": [
    "Next we use `METAWRAP`s bin refinement module to consolidate the bins we obtained from `CONCOCT`, `MAXBIN2`, and `METABAT2`. What `METAWRAP` does is, it creates first hybrid sets of bins from sets of existing bins. In our case we have bins from `METABAT2` (set A), , `MAXBIN2` (set B), and `CONCOCT` (set C), yielding four hybrid sets: `AB`, `AC`, `BC`, and `ABC`. The completion and contamination (covered below) is determined for each bin and the 7 sets of bins are then compared to each other. If a bin is identified in two sets (based on >80% genome overlap), the better version (based on completion and contamination) is kept. Contigs that are present in more than one bin are removed and only kept in the highest quality bin. The final set of refined bins is again checked for completion and contamination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386507de-bda1-4bd5-a281-9043d59d62f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Refining the bins\n",
    "metawrap bin_refinement -o ./02_ASSEMBLY_BINNING/bin_refinement_spades_metawrap -t 6 -A ./02_ASSEMBLY_BINNING/initial_binning_spades_metawrap/metabat2_bins/ -B ./02_ASSEMBLY_BINNING/initial_binning_spades_metawrap/maxbin2_bins/ -C ./02_ASSEMBLY_BINNING/initial_binning_spades_metawrap/concoct_bins/ -c 70 -x 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea129cff-b77c-48a5-8a5e-a37fa116307b",
   "metadata": {},
   "source": [
    "`-o`: specified output folder for the final set of refined bins    \n",
    "`-t`: we use multiple threads    \n",
    "`-A`-`-C`: set of originally binned bins to use for the refinement    \n",
    "`-c` & `-x`: cutoffs for genome completion (`-c`) and (`-x`) contamination    \n",
    "\n",
    "---\n",
    "‚ùì/üîß**QUESTION/TASK**\n",
    "\n",
    "How good are your bins, check the summary from the bin refinement module: `./02_ASSEMBLY_BINNING/bin_refinement_spades_metawrap/metawrap_70_10_bins.stats`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b178b113-3e09-40f1-bb88-203a4e59b4c2",
   "metadata": {},
   "source": [
    "### Human-augmented binning\n",
    "\n",
    "`METAWRAP` provides a very automated way for the binning process. Several other tools allow \"human-augmented binning\", which usually make use of differences in k-mer frequencies and differential coverage, and allow binning and refinement through visual inspection. Three examples are:\n",
    "\n",
    "* [`anvio`](https://anvio.org/)\n",
    "* [`VizBin`](https://github.com/claczny/VizBin)\n",
    "* [`BusyBeeWeb`](https://ccb-microbe.cs.uni-saarland.de/busybee/) ¬ª¬ª based on VizBin comes with a web interface\n",
    "\n",
    "OK, let's give `BusyBeeWeb` a try. \n",
    "\n",
    "1. We download our contigs and upload them to the `BusyBeeWeb`-Server,\n",
    "2. we request taxonomic annotation of our contigs,\n",
    "3. and run BusyBeeWeb.\n",
    "\n",
    "This should only take a few minutes.\n",
    "\n",
    "<img src=\"img/busybee_1.png\" alt=\"The BusyBeeWeb landing page.\" width=\"500\"/>\n",
    "\n",
    "<font size=\"2\"> The BusyBeeWeb landing page. </font>\n",
    "\n",
    "<img src=\"img/busybee_2.png\" alt=\"The upload mask.\" width=\"500\"/>\n",
    "\n",
    "<font size=\"2\"> The upload mask. </font>\n",
    "\n",
    "<img src=\"img/busybee_3.png\" alt=\"Exemplary results.\" width=\"500\"/>\n",
    "\n",
    "<font size=\"2\"> Exemplary results. </font>\n",
    "\n",
    "---\n",
    "‚ùì**QUESTION**\n",
    "\n",
    "How many bins did `BusyBeeWeb` find in comparison to `METAWRAP`?\n",
    "\n",
    "---\n",
    "#### About anvio\n",
    "\n",
    "`anvio` is extremely well-documented, and hardly matched by any other open source bioinformatics software project in terms of openness and service to the scientific community. If you are interested in genome-resolved metagenomics and just want to learn check out some `anvio`-related resources:\n",
    "\n",
    "* [`anvio homepage`](https://anvio.org/) ¬ª¬ª plenty of resources of meta'omics\n",
    "* [`Tara Oceans Genome-resolved metagenomics`](https://merenlab.org/data/tara-oceans-mags/) ¬ª¬ª story behind [this](https://doi.org/10.1038/s41564-018-0176-9) paper, complete walkthrough, great start if you want to familiarize yourself with `anvio`\n",
    "* [`Blog post about manual refinement`](https://merenlab.org/2017/05/11/anvi-refine-by-veronika/) ¬ª¬ª refinement matters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b4a5f5-ed69-455b-883f-1acf956f6878",
   "metadata": {},
   "source": [
    "## Session 02 | Taxonomic assignment of our refined bins\n",
    "\n",
    "Now that we are done with binning our genomes, we want to know _who are we dealing with?!_. We already got some idea about the taxonomic identity of our bins from `METAWRAP`, which uses under the [`checkM`](https://github.com/Ecogenomics/CheckM) to assess the completeness and contamination of bins based on single-copy core genes (SCGs). SCGs are \"genes that commonly occur across many taxa but only with one copy\". Contamination can for instance arise if we binned contigs that originated from different organisms. This is especially an issue when you are dealing with somewhat closely related organisms. \n",
    "\n",
    "---\n",
    "‚ùì**QUESTION**\n",
    "\n",
    "How can we use SCGs to assess/and determine:\n",
    "\n",
    "* genome completeness,\n",
    "* genome contamination,\n",
    "* and taxonomic affiliation?!\n",
    "\n",
    "---\n",
    "\n",
    "### Taxonomic assignment with GTDB-tk\n",
    "\n",
    "For the taxonomic assignment, we use [`GTDB-Tk`](https://github.com/Ecogenomics/GTDBTk) relies on the Genome Taxonomy Database [`GTDB`](https://gtdb.ecogenomic.org/) that is maintained by the Australian Center for Ecogenomics.\n",
    "\n",
    "`GTDB-Tk` is classifying genomes combining three approaches:\n",
    "\n",
    "1. Placing them in a reference tree based on a defined set of SCGs    \n",
    "¬ª taxon-specific SCG sets are selected based on a pre-screening    \n",
    "2. Calculating ANIs (average nucleotide identity) based on comparisons against reference genomes\n",
    "3. Determining relative evolutionary divergence\n",
    "\n",
    "GTDB-Tk uses big reference databases, meaning the run takes a while. Once it is done, check the output folder and check summary files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf2353a-c38c-4989-bd0b-fe5d9e380964",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3705eb-ecf5-4a1f-af2b-c4cd1187d1f5",
   "metadata": {},
   "source": [
    "Check out the `GTDB-tk` summary, available here `.../...`. What genomes are we dealing with?!\n",
    "\n",
    "---\n",
    "üîì**SUMMARY**\n",
    "\n",
    "* üéì we talked about how to taxonomically profile short-read metagenomic data, \n",
    "* üìà estimated coverage in our metagenome datasets,\n",
    "* üîç assembled them,\n",
    "* üéØ binned the assembled contigs,\n",
    "* üå≤ and checked their taxonomic affiliation!\n",
    "  \n",
    "--- \n",
    "\n",
    "<sub> ¬© Carl-Eric Wegner, 2023-08 </sub>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
